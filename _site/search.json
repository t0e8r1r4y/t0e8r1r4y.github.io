[
  
    {

      "title"    : "Hands-on.Create a tech blog",
      "url"      : "/git-blog",
      "content"  : "쉽지 않은 기술 블로그 만들기\n\n저는 스터디 그룹을 조그마하게 꾸리고있습니다. 스터디를 하면서 정리한 내용들을 블로그로 공개하고 ‘우리가 이런 일을 하고 이런거에 관심을 가지고 있어요.’라고 자랑하고 싶은(?) 니즈가 이미 오래전부터 있었습니다. 하지만 공통적으로 사용할 기술블로그를 만드는게 쉽지는 않았습니다.\n\n첫번째 시도는 노션 공개페이지를 활용하여 블로그를 만드는 것이었습니다. 노션은 사용하기 편리하고 스터디 구성원들 간 공유는 매우 편리하고 좋았습니다. 하지만 블로그 설명에 필요한 코드와 함께 연동하여 관리하기에는 다소 부족한 느낌이 있었습니다. 그리고 관리가 조금이라도 소홀해지면 사실 노트패드 같은 느낌으로 점점 쓰게되는 것 같았습니다. 완결성있는 블로그를 작성하기가 쉽지 않았습니다. 그런 의미에서 옵시디언도 같았습니다.\n\n두번째 시도는 노션에 작성한 글을 엮어서 미디엄 블로그를 만들어보자는 것이었습니다. 코드 관련된 부분은 여전히 어려움으로 남아있었지만, git 주소를 잘 남기고 관리한다면 보다 완결성있는 글을 쓸 수 있겠다고 판단하였습니다. 하지만 이 또한 어려움이 있었는데요. 노션에 비해서 문서 편집이 어려웠고, 코드를 삽입하는 것도 외부 의존성이 필요했습니다. 그리고 폰트나 꾸밈도 제한적이었습니다. 또 공통으로 사용할 구글 계정을 파고 해당 계정을 여러 환경에서 로그인을 시도하다보니, 보안적 이슈로 계정이 잠겨버리는 일도 있었습니다.\n\n\n\n\n노션 페이지를 그냥 블로그에 올려 버릴 수 없을까?\n\n여러 시도 끝에 가장 간단한 방법은 하나의 완성된 글을 노션에다 작성하고 이 페이지를 블로그에 바로 올려버리는 것이 간편하겠다는 생각이 들었습니다.\n\n노션은 스터디 구성원들이 수년간 사용한 경험이 있기에 다른 수단에 대한 러닝 커브가 가장 낮았습니다. 그리고 여타 다른 블로그들에 비해 api가 잘 되어있어 작성한 글을 자동으로 내려 받고, 이를 다른 블로그 플랫폼에 api를 호출하여 올리기에는 최적의 글쓰기 에디터라고 판단했습니다.\n\n이제 외부에 공개할 블로그 플랫폼을 선정하여야 했습니다. 티스토리, 브런치, 미디엄 등 다양한 블로그 플랫폼이 존재하였지만, 노션에서 추출한 내용을 업로드하기에 간편하다는 생각은 들지 않았습니다. 코드와 연동하여 가장 간편한 방법이 무엇일까 고민하였을 때 github blog가 최선이라는 생각이 들었습니다.\n\njeklly를 사용하면 github에서 모든 블로그 내용을 관리할 수 있고, 많은 공수없이 이미 만들어진 테마를 적용하여 빠르게 블로그 형태를 만들 수 있었습니다. 그리고 노션 api를 통해서 받아온 페이지 내용을 마크다운 파일 형태로 변경하여 블로그 포스트 경로에 추가해준 뒤 push만 하면 손쉽게 반영이 가능하여 발생을 위한 별도의 배포 파이프라인 구축도 불필요하였습니다.\n\n\n\n\n어떻게 구축할 것인가?!\n\n\n\n\n전체 그림은 위와 같습니다.\n\n\n  글 작성은 노션에서 진행합니다.\n  작성된 노션글은 파이썬으로 작성된 응용프로그램을 사용하여 api 호출을 통해 내용을 가져오고, 이를 마크다운으로 작성된 형태의 파일로 만듭니다.\n  현재는 로컬에서 git blog 프로젝트 경로에 포스트를 저장하는 경로에 저장하도록만 구현하였습니다. 그리고 로컬에서 미리 블로그 화면을 보고 검토를 한뒤 git에 push를 하도록 구축하였습니다.\n\n\n차후에는 운영룰을 정하고 응용프로그램을 aws와 같은 cloud 서비스에 올리므로서 자동으로 작성한 글이 post 되도록 할 예정입니다. 현재는 제 개인 기술 블로그를 위 내용으로 한번 구축해 봄으로서 필요한 내용을 아래에 정리하였습니다.\n\n\n\n\nGit Blog는 어떻게 만들 것인가\n\n제일먼저 필요한 것은 Git Blog의 테마라고 생각했습니다. 기본적으로 글을 어떻게 보여줄 것인지를 결정해야 했고, 노션에서 사용하는 강조 표시나 이런 부분들을 md 문법으로만 처리할 수 없기에 일부는 html style도 추가해야 했습니다. 이 부분에 대한 정확한 명세가 확정이 되어야 노션에서 추출한 게시글을 적절히 변경할 수 있다고 판단했습니다.\n\n\n\n\n\n저는 기본적인 목차형 블로그를 선택했습니다. 당연히 페이지 네이션도 적용을 해야된다고 생각했고, 글 상세 중 상단의 일부 내용을 보여 줄 수 있도록 만들었습니다. 몇개의 jeklly 테마를 조합해서 만들었습니다. 이미지의 경우 해당 레포지토리에 올릴 경우 용량의 한계가 있어 외부 cdn을 고려했습니다. 미디엄 블로그 등에 올린 포스트에서 주소만 잘 가져온다면 외부 블로그를 저장소로 사용할 수도 있다고 판단했고, 필요하다면 aws cloud front를 적절히 섞어 사용하면 될 것 같습니다.\n\n\n\n\n\n그리고 강조를 위한 부분은 div style을 별도로 정의하여 아래와 같이 만들었습니다. 이런 부분에서 front-end css에 대한 지식을 연마하는 기회도 되네요.\n\n\n\n\n\n그리고 코드와 관련된 부분은 highlight.js를 사용하였습니다. highlight.js는 적용도 간편하고 여러 디자인이 공개되어 있어 잘 적용한다면 아래와 같이 코드 가독성도 높일 수 있다고 생각했습니다.\n\n\n\n\n\n이렇게 페이지를 어떻게 노출할지 테마를 정하였다면 노션에서 데이터를 가져와서 변경을 할 준비가 되었습니다. 그리고 외부에 노출되는 것과 관련하여 ‘notion Google search console’ 설정을 해주어야합니다. 이를 통해 구글에서 검색 시 내 git blog 글이 노출될 되도록 합니다.\n\n\n\n\n응용 프로그램 개발에 필요한 것들\n\n응용 프로그램을 개발할 언어를 선택할 때 가장 중요하게 생각했던 부분은 api 호출을 간편하게 해줄 sdk나 lib가 공식적으로 있는지를 중요하게 생각했습니다. 노션 developer 공식 문서에는 js로 예제들이 적혀 있었지만 개인적으로 python 공식 sdk가 사용이 훨씬 편리하다고 판단하여 python으로 응용프로그램을 개발하기로 결정하였습니다.\n\n\n\n\n\n노션 api를 사용하기 위해서는 Integration 설정이 필요합니다. 노션 developer에서 키를 발급받아서 프로그램에서 api 호출 시 사용하면 됩니다. 그리고 페이지를 추출하기 위해서는 완결된 글 페이지에서 해당 Integration을 연동하는 작업도 필요합니다.\n\n\n\n\n\n위 작업을 완료한 뒤 api로 데이터를 호출하고 응답값을 분석하여 git blog에서 정의한 스타일에 맞게 적절히 변경을 해주어야 합니다. (api 분석에 대한 부분은 노하우가 생기면 별도의 글로 올리겠습니다.) 노션의 콜아웃 같은 경우 md로 적절히 변경하기 어렵기 때문에 응용 프로그램에서 적절히 변경을 해야합니다. 이런 변경이 완료되면 해당 파일을 저장할 때 git blog 프로젝트에서 포스트 내용을 저장하는 경로에 저장을 하였습니다. 이를 push하여 블로그에 글이 자동으로 반영되도록 하였습니다.\n\n\n\n\n어떤 운영룰을 만들면 좋을까?\n\n저는 작성한 글에 대한 리뷰를 운영룰을 정하는게 중요하다고 생각합니다. 공통으로 사용하는 블로그라면 전체적인 주제의 방향성을 유지하는 것도 중요하고, 코드 리뷰 처럼 게시글을 꼼꼼하게 리뷰하면서 서로의 역량을 강화하는 것도 중요하겠지요.\n\n\n\n\n\n블로그에 공개되는 내용은 _posts에 저장을 하고있는데 리뷰를 위해 _publication 경로를 만들어서 바로 올리지 않고, 한번 검수를 거쳐가도록 만들면 좋을 것 같습니다. PR을 올리면 글을 읽어보면서 바로바로 리뷰를 달고 이를 수정하여 _posts로 옮기면 좀 더 완성도 있는 글이 완성될 것 같습니다.\n\n그리고 차후에는 글감도 관리는 함께한다면 좀 더 완성도 있는 블로그가 되지 않을까요?\n\n\n\n\n마치며\n\n스터디 기술블로그를 구축하기 위한 고민과, 이를 먼저 개인블로그에 적용시켜보면서 얻은 내용을 간략하게 정리하였습니다. 해당 내용이 좀 더 발전이 된다면 해당글을 보완하고자 합니다.\n\n최근에는 스타트업 등에서도 기술력을 알리고자 이런 블로그를 만들고 관리하는 추세입니다. 이런 내용이 조금이나마 도움이 될 수 있으면 좋겠습니다."

    },
  
    {

      "title"    : "Hands-on.Add a login method",
      "url"      : "/login-part-two",
      "content"  : "로그인 프로세스 만들기\n\n문제정의\n목표\n\n  사용자가  로그인을 하기 편리한 프로세스를 확립한다. 가급적 한번의 버튼 클릭으로 로그인을 할 수 있다.\n  데이터 분석 목적으로 사용자의 정보를 최대한 많이 수집할 수 있는 로그인 수단을 고려한다.\n  하나의 Auth 서버를 사용하여 여러개의 서비스를 로그인 할 수 있도록 설계한다.\n\n\n기대효과\n\n  사용자는 한번의 버튼 클릭으로 로그인을 할 수 있다.\n  운영 관점에서 비용을 최소화 한다.\n  사용자 정보를 수집하고 이를 분석하여 서비스를 개선할 수 있도록 한다.\n\n\n\n개략적인 설계안 작성하기\n사용자가 로그인하기 편리한 프로세스\n개략적인 설계안\n\n  사용자가 한번의 버튼 클릭을로 로그인을 하기위해서 로그인 절차와 회원가입 절차를 구분하여 설계할지 하나의 절차로 설계할지 의사결정이 필요하다.\n    \n      최근 서비스들의 UX를 보면 외부 Third Party Login을 제공하는 벤더사들을 활용하는 경우 로그인과 회원가입 절차를 한 흐름으로 가져간다.\n      이메일 인증을 통해서 회원가입을 하는 경우에는 별도의 회원가입 화면이 표시되고 이메일 인증이 되면 로그인으로 이어지는 흐름을 제공한다.\n    \n  \n  \n    개략적인 흐름을 간단하게 표현하면 아래와 같다.\n\n  \n  처리 흐름\n    \n      Web이던 App이던 Third Party Auth에 인증정보를 요청\n      Third Party Auth에서는 인증정보 응답\n      Client는 서버로 Third Party Auth에서 받은 정보 전달\n      Server는 Third Party Auth에 Client에서 받은 정보를 확인\n      검증 결과 응답\n      검증 결과에 따라 Clinet에 Server가 적절한 응답\n    \n  \n\n\n\n데이터 분석목적으로 데이터 수집하기\n개략적인 설계안\n\n  인구 통계적 관점에서 데이터 수집\n    \n      이름, 생년월일, 주소 등\n    \n  \n  사용자와 커뮤니케이션 목적의 데이터 수집\n    \n      이메일, 전화번호\n    \n  \n  Third Party Login을 사용한다면 아래의 정보를 수집할 수 있다.\n    \n      정보 제공 항목 ( 서버에서 요청으로 가져올 수 있는 항목 - 해당 정보는 각 디벨로퍼 센터에서 확인이 필요함 )\n\n    \n  \n\n\n고려사항\n\n  개인정보는 한 개인을 식별할 수 있는 고유한 정보로서 법적으로 관리가 필요한 이슈가 발생한다. 따라서 위 내용 중 관리의 이슈가 생기는 부분에 대해서는 사전고지와 해당 데이터를 관리할 책임자가 필요하다.\n  사이드 프로젝트에서 사용할 것이므로 최소한으로 수집할 수 있도록 데이터를 설계한다.\n\n\n\nAuth 서버 공통으로 사용하기\n개략적인 설계안\n\n\n\n  여러 서비스에서 하나의 Auth를 쓸 수 있도록 한다.\n    \n      여러 서비스를 동시 다발적으로 개발하여 시장에서 테스트를 하고 반영할 수 있도록 하기 위함이다.\n    \n  \n  초기 설계 구현 시 사용할 인프라\n    \n      AWS Lambda를 서버로 두고 AWS에서 인프라 사용\n      보안상 필요한 것들은 WAF, SHEILD 등 함께 사용\n참고자료\n    \n  \n\n\n\n관련 기술 조사\nOAuth\n정의\n\n  Open Authorization의 약자로 인터넷 사용자들이 비밀번호를 제공하지 않고, 다른 웹사이트상의 자신들의 정보에 대해 웹사이트나 애플리케이션의 접근권한을 부여할 수 있는 공통적인 수단으로 사용되는, 접근 위임을 위한 개방형 표준\n\n 인증이 아니라 인가를 제공하는 표준화된 규약이다.\n\n불특정 다수의 클라이언트가 직접 인증을 구현하지 않고 네이버나 구글(인증 제공자)의 인증에 의존. OAuth2.0를 활용해서 이러한 인증서비스(네이버 아이디로 로그인, Google로 로그인)를 제공하고 있으며 SSO와 비슷한 효과를 얻을 수 있다. 그러나 OAuth와 SSO는 절대 같은 의미가 아니다. \n참고자료\n\n  OAuth 2.0.을 구성하는 4가지\n    \n      Resource Owner : 리소스 소유자. 리소스 서버에 접근할 수 있도록 자격을 부여하는 주체\n      Client : 사용자가 사용하는 애플리케이션(웹, 앱의 형태)\n      Authorization Server : 권한을 관리해주는 서버. 클라이언트의 자격 접근을 확인하고 Access Token을 발급하여 권한을 부여하는 역할\n      Resource Server : 사용자의 보호된 자원을 관리하는 서버. AccessToken을 받고 응답을 제공함\n    \n  \n  사용하는 토큰의 종류\n    \n      Access Token : 사용자의 데이터에 접근하기 위해 필요한 자격증명. 특정 Client에 부여한 권한에 대한 정보가 담긴 문자열\n      Refresh Token : Access Token이 만료되었을 경우 이를 재발급하기 위한 목적.\n      보안의 목적으로 Access Token은 만료기간이 짧고, Refresh Token은 상대적으로 길다.\n    \n  \n\n\n인증 방식의 종류\n\n  Authorization Code Grant(권한 부여 승인 방식)\n    \n      흐름도\n\n    \n  \n  Implicit Grant(암묵적 승인 방식)\n    \n      흐름도\n\n    \n  \n  Resource Owner Password Credentials Grant(자원 소유자 자격 증명 승인 방식)\n    \n      흐름도\n\n    \n  \n  Client Credentials Grant(클라이언트 자격 증명 승인 방식)\n    \n      흐름도\n\n    \n  \n\n\n관련 링크는 아래와 같다.\n참고자료\n\n 웹에서는 주로 1번 방식을 사용한다. 웹의 쿠키나 캐시는 탈취가 가능하기 때문이다. \n 앱서비스는 2,3,4의 흐름을 주로 가진다. 앱 자체에서 사용되는 sdk에서 Thrid party login을 제공하는 벤더사들 Auth 서버와 통신을 하여 이미 클라이언트 레벨에서 인증이 완료가 되고 그 인증 정보를 가지고 서버는 다시금 확인 절차만 가져가기 때문이다. \n\nOAuth 로그인을 제공하는 회사들의 인증 방식 및 제공정보\n\n  주요 로그인 수단\n    \n      네이버\n        \n          네이버 디벨로퍼 센터\n참고자료\n        \n      \n      카카오\n        \n          카카오 디벨로퍼 센터\n참고자료\n        \n      \n      페이코 인증 서비스\n      애플\n        \n          다른 회사와 다르게 OAuth 인증 Flow가 조금 다르다. 위 조사한 종류에서 암묵적 인증 방식을 따르는 것으로 보인다.\n          그래서 개인 프로젝트에서는 firebase auth에 수단을 추가하고 그것을 기준으로 처리하는 흐름으로 가져갔다.\n        \n      \n      구글\n        \n          특징\n            \n              rest api와 firebase auth의 인증방식이 유사하기에 관련된 값들만 있다면 두 인증방식을 동시에 사용 할 수 있다.\n              사용하기에 한국어 레퍼런스가 많아서 손쉽게 정보를 구하고 쓸 수 있다.\n              스프링부트 프로젝트에서 firebas auth dependenciies는 아래와 같이 추가할 수 있다.\n              문서가 구글 디벨로퍼, 파이어베이스 문서 등 내용이 조금 산재되어있고, OAuth 관련 내용의 이해가 조금 부족하다면 혼선이 올 수 있다.\n            \n          \n          파이어 베이스\n참고자료\n          구글 디벨로퍼 센터\n참고자료\n        \n      \n      페이스북(메타)\n        \n          메타 디벨로퍼 센터\n참고자료\n        \n      \n      라인\n        \n          라인 디벨로퍼 센터\n참고자료\n        \n      \n    \n  \n  특징\n    \n      국내 제공 회사들은 조금 더 많은 사용자 정보를 제공하는 경향이 있다. 물론 사용자 정보 제공 동의를 하지 않는다면 클라리언트나 서버에서 조회할 기회는 없지만, 사용성을 고려했을 때 노출되는 빈도가 훨씬 높다.\n      회사들 마다 차이는 있지만 Refresh Token 관리 정책은 차이가 존재한다. 해당 정보를 모든 Response 마다 제공하지 않거나, 관련 처리 자체를 자기네들이 하는 경우도 있다.\n      대부분 로그인 수단을 구현해본바, 로그인 프로세스 설계(기획적인 절차 포함)에 따라 적절히 관련 기술을 활용하면 된다.\n    \n  \n\n\n\n\n더 알아보면 좋은 웹 보안 SSO\n\n\n주요 용어 정리\n\n  SSO : Single Sign On\n  SAML : Security Assertion Markup Language\n  OAuth2 : Open Authorization 2\n\n\nSSO의 장점\n\n  한 세트의 인증정보만으로 연관된 모든 사이트에 접근할 수 있기때문에 보안관리 측면에서 사용자경험을 개선\n  기존에 여러 곳에서 분산되어 관리되던 인증정보가 한 곳으로 집중되기 때문에 보안을 강화하기가 수월\n\n\nCookie 기반 SSO\n\n  과거엔 Cookie를 이용하여 SSO를 구현했다고 한다. 하지만 여러가지 한계점이 존재했다.\n\n\nSAML이란 ?\n\n  \n    IdP(ID 공급자)와 SP(서비스 공급자) 간 인증정보를 교환하기 위한 마크업 언어\n\n  \n  항공기 탑승과정을 통해 이해하는 SAML\n  항공기에 탑승하기 전에 항공사는 다른 승객의 보안을 보장하기 위해 귀하가 누구인지 확인해야 합니다. 따라서 정부에서 발급한 신분증으로 신원을 확인합니다. 신분증의 이름이 항공권의 이름과 일치하는지 확인하면 비행기에 탑승할 수 있습니다.\n  위의 예에서 정부는 ID 제공자이고 항공사는 서비스 제공자입니다. \n  정부에서 발급한 신분증은 SAML Assertion입니다. 정부 ID를 신청할 때 일반적으로 양식을 작성하고 사진을 찍어야 하며 경우에 따라 지문도 찍어야 합니다. 그런 다음 정부(ID 제공자)는 이러한 식별 속성을 데이터베이스에 저장하고 귀하의 신원과 관련된 실제 ID를 발급합니다. \n  탑승구에 도착하면 항공사(서비스 제공업체)가 사용자의 신분증( SAML Assertion )을 확인합니다. 항공사는 귀하의 세부 정보가 포함된 귀하의 신분증을 받아 유효성 검사를 실시합니다. 모든 절차가 끝나고 인증에 성공하면 항공사(서비스 제공업체)에서 항공기 탑승(서비스 이용)을 허용합니다.\n\n\nSAML SSO의 워크플로우 이해\nIdP(ID 공급자) Initiated\n\n\n\n  사용자는 IdP에 먼저 접속 및 로그인\n  인증 완료시 IdP는 SP에게 SAML Response 전달\n\n\nSP(서비스 공급자) Initiated\n\n\n\n  사용자가 SP의 서비스를 이용하기 위해 Access\n  SP는 사용자가 현재 미인증 상태임을 확인하고 IdP로 User를 Redirect(이때 사용자는 SAML Request를 하게 됨)\n  IdP는 필요한 경우 사용자에게 자격증명을 요청 (이미 인증된 경우 생략)\n  사용자는 IdP에 로그인\n  IdP는 SAML Response을 생성하여 사용자의 브라우저에 반환합니다.\n\n\n\n\n\nSAML 예제 코드 분석\n\n  IdP\n    \n      SamlIdpApplication : 메인, 진입점\n      WebSecurityConfigurer : Spring Security 설정 파일\n    \n    \n  \n  설정 내용 \n  - 디폴트 유저 생성(DB가 따로 없음)\n  - 페이지별 권한 세팅(스프링 시큐리티 기본기능)\n  - SAML 응답을 위한 Filter 추가\n  - KeyStoreLocator를 통해 동적으로 KeyStore 생성\n  \n\n  \n  AbstractSamlPrincipalFactory, LocalSamlPrincipalFactory, SamlPrincipal\n    \n  \n    //SP에게 전달받은 authnRequest(인증요청)으로부터 \n    //SamlPrincipal을 생성하는 역할\n\n    public SamlPrincipal createSamlPrincipal(@SuppressWarnings(\"rawtypes\") SAMLMessageContext messageContext,\n                                            Authentication authentication) {\n        AuthnRequest authnRequest = (AuthnRequest) messageContext.getInboundSAMLMessage();\n        List&lt; SamlAttribute&gt; attributes = createAttributes(authentication);\n        return SamlPrincipal.builder(authentication.getName(), nameIdType, attributes)\n                            .serviceProviderEntityID(authnRequest.getIssuer().getValue())\n                            .requestID(authnRequest.getID())\n                            .assertionConsumerServiceUrl(authnRequest.getAssertionConsumerServiceURL())\n                            .relayState(messageContext.getRelayState())\n                            .build();\n    }\n    //템플릿 메소드 패턴을 이용하여 Saml속성을 Create하는 부분을 추상화.\n    protected abstract List&lt; SamlAttribute&gt; createAttributes(Authentication authentication)\n  \n\n  \n  SamlMessageHandler\n    \n  \n    //핵심역할 : authnRequest(인증요청)을 파싱, SamlResponse를 전송\n\n    //실제 SamlResponse의 내용을 생성하는 역할은 SamlBuilder가 한다. \n    import static saml.example.idp.SamlBuilder.buildAssertion;\n    import static saml.example.idp.SamlBuilder.buildIssuer;\n    import static saml.example.idp.SamlBuilder.buildSAMLObject;\n    import static saml.example.idp.SamlBuilder.buildStatus;\n    import static saml.example.idp.SamlBuilder.signAssertion;\n\n    //시그니쳐만 명시\n    public SAMLMessageContext extractSAMLMessageContext(HttpServletRequest request,\n                                                            HttpServletResponse response)\n                throws ValidationException, SecurityException, MessageDecodingException\n\n    //시그니쳐만 명시\n    public void sendAuthnResponse(SamlPrincipal principal, HttpServletResponse response)\n                throws MarshallingException, SignatureException, MessageEncodingException\n  \n\n  \n  SamlResponseFilter\n    \n  \n    //역할: 인증워크로드 정의 \n    protected void doFilterInternal(HttpServletRequest request,\n                                    HttpServletResponse response,\n                                    FilterChain filterChain) throws ServletException {\n        try {\n            if (!request.getRequestURI().startsWith(ssoUrl)) {\n                filterChain.doFilter(request, response);\n                return;\n            }\n            Authentication authentication = SecurityContextHolder.getContext().getAuthentication();\n            if (isNull(authentication)\n                    || authentication instanceof AnonymousAuthenticationToken\n                    || !authentication.isAuthenticated()) {\n                filterChain.doFilter(request, response);\n                return;\n            }\n            @SuppressWarnings(\"rawtypes\")\n            SAMLMessageContext messageContext = samlMessageHandler.extractSAMLMessageContext(request, response);\n            SamlPrincipal principal = samlPrincipalFactory.createSamlPrincipal(messageContext, authentication);\n            samlMessageHandler.sendAuthnResponse(principal, response);\n        } catch (Exception e) {\n            throw new ServletException(\"Failed to send saml response.\", e);\n        }\n    }\n  \n\n  \n  SP\n    \n      SamlSpApplication : 메인, 진입점\n      WebSecurityConfigurer : Spring Security 설정 파일\n    \n    \n  \n    설정 내용 \n    - 페이지별 권한 세팅(스프링 시큐리티 기본기능)\n    - 인증 제공자로 SamlAuthenticationProvider 지정\n    - 인증 진입점으로 SamlSsoEntryPoint 지정\n    - SamlAssertionConsumeFilter를 보안필터로 추가\n  \n\n  \n  SamlSsoEntryPoint  : 진입점\n    \n  \n  //역할 : SAML 요청을 생성한 후 IdP로 Redirect 시킴 \n  //redirect URL \n  // http://localhost:9105/sso?SAMLRequest=fZFfa8IwFMXf9ylK3mObVGsNtuI2ZIJjRese9pbG6wy0ictNZfv2U6swYfh4L%2BfcP78znnw3dXAAh9qajLBeRAIwym60%2BczIupzRlEzyhzHKpuZ7MW39zizhqwX0wRQRnD%2F6nqzBtgG3AnfQCtbLRUZ23u9FGNZWyXpn0YsRi5JQKiTB%2FDkjMYvTJN0CTUEmtN8fDmg1YIxWCiKWxsMtqOqoxEIi6gNkxLsWTg1sYW7QS%2BMzwiPOKIsoY2XUF4NY8GEvGfEPEhTOeqts%2FahN90nrjLASNQojG0DhlVhNXxeC9yJRdSIUL2VZ0OJtVZLg%2FUqEn4gcGRkUHYP7s%2FaXxSTvkInzxS6YWddIf9976ugN3Z6lAozX%2Fudm9327vMZB8n%2Fhj8O%2FF%2BWX8jbT%2FBc%3D\n  \n\n  \n  SamlAssertionConsumeFilter\n    \n  \n    //역할 : SAML 응답을 통해 인증을 처리하는 워크로드 정의\n    //IdP로 부터 이미 응답을 전달 받은 상태\n    @Override\n    public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response)\n            throws AuthenticationException {\n        LOGGER.debug(\"Attempt authentication...\");\n        //samlContextProvider를 통해 samlContext 가져옴\n        SamlContext samlContext = samlContextProvider.getLocalContext(request, response);\n        SamlPreAuthenticationToken token = new SamlPreAuthenticationToken(samlContext);\n        return getAuthenticationManager().authenticate(token);\n    }\n  \n\n  \n  SamlAuthenticationProvider\n    \n  //역할 : 인증 로직 처리\n    @Override\n    public Authentication authenticate(Authentication authentication) throws AuthenticationException {\n        SamlPreAuthenticationToken preAuthenticationToken = (SamlPreAuthenticationToken) authentication;\n        SamlContext samlContext = preAuthenticationToken.samlContext();\n        @SuppressWarnings(\"rawtypes\") SAMLMessageContext messageContext = null;\n        try {\n            messageContext = extractSAMLMessageContext(samlContext.request());\n        } catch (MessageDecodingException | SecurityException e) {\n            LOGGER.error(\"Failed to decode saml request\", e);\n            throw new InternalAuthenticationServiceException(\"Failed to decode saml request\", e);\n        }\n        //saml 응답 가져오기 \n        Response samlResponse = (Response) messageContext.getInboundSAMLMessage();\n          \n        //로그인 성공 여부 확인\n        String statusCode = samlResponse.getStatus().getStatusCode().getValue();\n        if (!StatusCode.SUCCESS_URI.equals(statusCode)) {\n            LOGGER.error(\"SAML login failed. status code[{}]\", statusCode);\n            throw new AuthenticationServiceException(\"SAML response status fail, code[\" + statusCode + \"]\");\n        }\n        //assertionConsumer를 통해 userDetail 생성\n        UserDetails userDetails = assertionConsumer.consume(samlResponse);\n        LOGGER.info(\"Login user[{}]\", userDetails);\n        List&lt; GrantedAuthority&gt; authorities = AuthorityUtils.createAuthorityList(\"ROLE_USER\"); // for test!!\n\n        //인증완료 처리\n        SamlAuthenticationToken resultToken = new SamlAuthenticationToken(userDetails.getUsername(), authorities);\n        resultToken.setAuthenticated(true);\n        resultToken.setDetails(userDetails);\n\n        return resultToken;\n    }\n  \n\n  \n  SimpleSamlAssertionConsumer\n    \n  \n    //역할 : SAML 응답을 전달받아 UserDetail 생성 \n    public UserDetails consume(Response samlResponse) throws AuthenticationException {\n        validateSignature(samlResponse);\n        checkAuthnInstant(samlResponse);\n        Assertion assertion = samlResponse.getAssertions().get(0);\n        LOGGER.debug(\"Assertion[{}]\", SamlUtil.samlObjectToString(assertion));\n        return createUser(assertion);\n    }\n  \n\n  \n\n\n\n추가 고려사항\nFlutter SDK를 사용하지 않고 Native로 구현할 수 있을까?\n\n  물론 가능하다. api를 요청하는 방식으로 OAuth 1번 방식대로 처리할 수 있지만, 해당 구현로직을 안드로이드 IOS 모두 구현하는 것 자체가 시간이 소요될 수 있어 의사결정이 필요한 부분이라 판단함\n\n\n\n관련 Redirect URL 처리는 어떻게 할 수 있을까?\n\n  Flutter SDK에서 컨트롤하는 웹뷰 페이지에 대해서는 직접 컨트롤 할 수 있는것도 없는 것도 있음\n  이걸 컨트롤하는 것이 현실적으로 쉽지 않기 때문에 SDK를 사용하는 것이 편리할 수 있음\n\n\n로그인 실패관련 로깅은 어떻게 하면 좋을까?\n\n테스트 작성 시 무엇을 고려하면 좋을까?\n\n  테스트의 어려움\n    \n      외부 의존적인 테스트가 진행되어야 하기에, 테스트 흐름이 까다로움 → 해당 부분만 Mock Server를 두는 방식으로 처리하는 방식 등 염두\n    \n  \n\n\n프로젝트 설정 시 고려 사항\n\n  프로젝트를 설정할 때는 dev, stage, prod 등 환경별로 나눠서 처리하도록 한다.\n    \n      가령 Firebase auth를 사용한다고하면 SSO 등을 제공하는데 이러한 부분들을 고려했을 때 환경별로 구분을 해두는 것이 유용해 보임"

    },
  
    {

      "title"    : "Hands-on.Write procedures well in postgresql",
      "url"      : "/postgresql-procesure",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  프로시저에 대해서\n  업무에서 효율적으로 쓴 경험"

    },
  
    {

      "title"    : "Hands-on.Push Service. How to Do that?",
      "url"      : "/push-architecture",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  푸시에 대해서\n  푸시를 보내는 여러가지 방법 서칭"

    },
  
    {

      "title"    : "Hands-on.Getting started with Flutter",
      "url"      : "/flutter",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  앱개발\n  플러터의 이해"

    },
  
    {

      "title"    : "Hands-on.Manage image resources well with S3 and CloudFront and Waf",
      "url"      : "/images",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  s3 이미지 처리 관련 경험담\n  어찌하면 좋을까?"

    },
  
    {

      "title"    : "Hands-on.Improve Login Process",
      "url"      : "/login-part-one",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  로그인 개선기\n  로그인에 대해서"

    },
  
    {

      "title"    : "Theory.Is the CDC &amp; Debeziumev necessary for our organization?",
      "url"      : "/cdc-debezium",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  CDC란?\n  솔루션\n  사례\n  우리조직에 필요할까?"

    },
  
    {

      "title"    : "Hands-on.Introduction to React programming",
      "url"      : "/react-programing",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  프론트엔드 개발을 시작하면서\n  좌충우돌 front 개발기\n  제일먼저 찾아본 지식들\n  앞으로 프론트를 어떻게 해나갈 것인가?\n  feat. full stack이란 무엇인가?"

    },
  
    {

      "title"    : "Hands-on.Extract statistical information with JPA QueryDsl",
      "url"      : "/query-dsl",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  QueryDsl에 대해서\n  실무 이야기 잠깐해주고\n  QueryDsl 사용하면서 꿀팁\n  통계정보에 QueryDsl 적용\n  후기 장단점"

    },
  
    {

      "title"    : "Hands-on.Building AWS Infrastructure with Terraform",
      "url"      : "/terraform-first-copy",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  aws solution 중 사용한 것들과 그것들에 대한 특징 정리\n  terraform set up 요약\n  인프라 구조\n  적용하고나서 느낀점"

    },
  
    {

      "title"    : "Organizational Culture.Agile Process in Dev Org",
      "url"      : "/agile",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  에자일에 대하여\n  우리 조직이 사용하는 프로세스와 솔루션 소개\n  나의 생각"

    },
  
    {

      "title"    : "Paper Reading. Apache Druid - A Real-time Analytical Data store",
      "url"      : "/apache_druid-paper-reading",
      "content"  : "1. 드루이드 개발 목적\n\n\n1-1.드루이드란? ( 논문의 결론 부분 )\n\n드루이는 다음을 충족하는 데이터 스토어이다.\n\n\n  분산\n  컬럼 지향형\n  리얼타임 분석 데이터 스토어 (처리하는 데이터를 drill-down하게 aggregates 할 수 있도록 설계 됨)\n  고성능, 쿼리 지연이 낮도록 설계 됨.\n  스트리밍 데이터 처리와 fault-tolerant를 지원함.\n\n\n\n\n1-2. 드루이드 개발 목적 ( 논문의 도입 부분과 문제 정의 부분 )\n\nDruid was originally designed to solve problems around ingesting and exploring large quantities of transactional events (log data). → OLAP\n\n드루이드는 분산,  컬럼 지향형, 리얼타임 분석 데이터 스토어 ( 쿼리 수행 속도가 더 빠른 )를 개발하고자 만들었다.\n\n이는 기존의 하둡이 혁신적이고 데이터 저장과 대용량의 데이터에 대한 접근은 우수하지만, 데이터에 대한 접근 속도(실시간 쿼리 등), 병렬로 수많은 데이터가 쏟아지는 경우 드루이드 개발자들의 니즈에 충족되지 못하였다고 함. ( Hadoop wasn’t going to meet our needs ).\n\n그리고 NoSql 아키텍처나 다른 솔루션들도 개발진이 원하는 수준이 아니거나, 비용타당성이 떨어짐.\n\n\n\n이러한 메타데이터는 Timestamp가 있고, Dimension 컬럼이 많고, Metric 컬럼이 많다.\n\n\n  원문  \n  This metadata is comprised of 3 distinct components. First, there is a timestamp column indicating when the edit\n  was made. Next, there are a set dimension columns indicating various attributes about the edit such as the page that was edited, the\n  user who made the edit, and the location of the user. Finally, there\n  are a set of metric columns that contain values (usually numeric)\n  that can be aggregated, such as the number of characters added or\n  removed in an edit.\n\n\n드루이드 개발진은 이러한 데이터를 drill-down하게 계산하고, aggregates 하는 것을 목표로 하였다. 그래서 논문의 결론을 한번더 언급하지만\n\n\n  분산\n  컬럼 지향적\n  리얼타임 데이터 분석 ( 리얼타임으로 drill-down하게 계산하고, 집계(agrregates) 하는 데이터 스토어\n\n\n이런 것을 만들고자 Druid를 만들었다.\n\n\n\n\n2. 드루이드 아키텍처\n\n\n\n2-1. Real-time Nodes\n\nReal-time Nodes는 실시간 데이터를 흡수하는 계층이면서 스트림에 쿼리 이벤트를 수행하는 노드라고 보임. ( 흡수하는 계층이면서 데이터 컨슈머 역할을 함 )\n\nReal-time Nodes에서는 이벤트들을 흡수하고 실시간 쿼리가 가능한데, 이를 위해서 in-memory index 버퍼를 유지함. ( 그리고 이를 위해서 JVM 내부 힙 기반의 버퍼를 사용한 row store를 가지고 있다고 하는데 요대목이 조금 이해가 안됨 )\n\n그리고 유지하는 index는 주기적으로 디스크에 유지하여 불변성을 유지함. 그리고 요때 컬럼지향형으로 데이터가 converting 된다.\n\n디스크에 저장된 index는 불변이고, 이를 논문에서는 persisted-indexer라고 명명함. 그리고 disk에 저장된 index들을 Off-heap memory에 불러온다.\n\n논문에서 제시한 도식처럼\n\n쿼리가 Real-time Node에 들어오면( Events indexed via these nodes are immediately available for querying ) in-memory index와 Off-heap memory and persisted indexex 둘 다 searching 해서 결과를 return한다.\n\n\n\n그리고 Real-time nodes는 일정 기간동안 영구적으로 저장된 index들을 하나의 불변의 block으로 묶고 ( 이를 segment라고 명명함 ) 영구적인 백업스토리지에 업로드를 한다.\n\n이런 백업 스토리지는 S3나 HDFS와 같은 파일시스템을 주로 쓰게되며, 이러한 행위를 드루이드에서는 deep storage라고 한다.\n\n이 과정을 요약하자면\n\n\n  실시간 스트리밍 데이터를 흡수하여 in-memory-index를 생성\n  in-memory-index를 컬럼 지향형 포맷으로 converting하고 Disk에 저장\n  일정 기간(some span of time) 모인 데이터를 묶어서 segment 생성\n  이 세그먼트를 백업 스토리지 저장 → deep store\n\n\n이 과정을 다시 논문 도식에 추가해보면…\n\n\n\n(데이터 indexing 방법에 대한 일반적인 내용도 조금 참고가 필요함)\n\n검색 데이터 색인 방법론\n\n\n\n2-1-1. Availability and Scalability\n\n위에서 Real-time-node는 컨슈머라는 표현을 썻는데… 논문의 맥락상 그 이야기는 여기 나온다. 컨슈머는 별도의 데이터 스트림을 생성해 줄 프로듀서가 필요하다.\n\n주로 이럴 때 kafka를 쓴다. 데이터 버스 역할로 카프카를 쓰는 이유는 데이터 버퍼의 역할과 데이터 손실 났을 때 카프카 broker에 저장 된 데이터 소스에서 데이터를 가져다 쓰기 때문에 손쉬운 복구가 가능하다는 의미로 보인다.\n\n\n\n논문에서 제시하는 카프카 + 드루이드 조합의 성능은 원문 그대로를 가지고 왔다\n\n🔥 원문 - In practice, this model has allowed one of the largest production Druid clusters to be able to consume raw data at approximately 500 MB/s (150,000 events/s or 2 TB hour).\n\n\n2-2. Historical Nodes\n\nHistorical Nodes는 불변 데이터의 읽기 일관성을 제공하는 노드다.\n\n\n\n해당 노드는 Deep Storage로 부터 세그먼트를 다운로드하거나 메모리로 로드하는 역할을 한다. Real-time-nodes 설명에서 동작하는 과정에서 위의 역할을 해주는 것이 historical nodes다.\n\n\n\n2-2-1. Tiers\n\nTiers는 Historical nodes의 그룹으로 정의한다. 티어들은 각각의 티어단위로 설정되는데,\n\nDifferent performance and fault-tolerance parameters can be set for each tier\n\n라고 원문에서 밝힌다. ( 원문을 한글로 더 좋게 번역을 못하겠음 )\n\n이렇게 하는 목적은 중요도에 따른 세그먼트들의 높고 낮은 우선순위 때문이다. 논문에서는 예시를 들어서 설명했는데, 액세스가 더 많이 되는 세그먼트들에게 더 큰 캐쉬 메모리를 할당하기 위한 목적 정도로 간단히 이해하면 될 듯.\n\n\n\n2-2-2. Availability\n\nHistorical node는 주키퍼를 통해 세그먼트를 로드 하거나 언로드하는 명령을 받는다. 그래서 주키퍼가 unavailable 상태가 되면 더 이상 새로운 세그먼트는 받지 못함. 그러나 서빙은 가능함.\n\nThis means that Zookeeper outages do not impact current data availability on historical nodes.\n\n\n\n2-3. Broker Nodes\n\nBroker Nodes는 real-time nodes와 historical nodes 사이에서 ‘쿼리 라우터’ 역할을 한다. 두가지 nodes 노드에게 질의하여 결과를 merge하는 역할을 하고 있다.\n\n\n\n2-3-1. Caching\n\n브로커 노드는 LRU 캐시 전략을 사용한다. ( 캐시 전략은 별도 링크 )\n\n\n\n\n  Broker nodes는 쿼리를 받으면 일단 세그먼트 셋에서 먼저 일치하는 것이 있는지 확인\n  일치하는 것이 있다면 다시 계산할 필요 없음\n  일치하는 것이 없다면, broker nodes는 historical nodes와 real time nodes에  쿼리를 각각 날림\n  broker nodes는 historical nodes에서 온 결과 값이 있으면 이를 cache로 저장 ( 이렇게 하는 이유는 real time nodes는 perpetually changing and caching the results is unreliable 하기 때문.\n\n\n\n\n2-3-2. Availability\n\n상태를 관리하는 주키퍼가 unavailable하게 되어도 쿼리가 가능하다. 기존에 클러스터와 통신이 정상이라고 하면 쿼리 수행을 하기 때문.\n\n\n\n2-4. Coordinator Nodes\n\nCoordinaotr Nodes는 드루이드 아키텍처 내부에서 ZooKeeper와 같은 역할을 하는 nodes이다. Historical nodes들의 분산 코디네이터 역할을 하며, historical nodes와 주키퍼 사이에서  load new data, drop outdated data, replicate data, and move data to load balance 이런 명령을 전달하는 역할을 한다.\n\nCoordinator Nodes들은 주키퍼에 의해 메인이 선출되고 나머지는 backup의 형태로 동작함\n\nCoordinator Nodes들은 주기적으로 현재 클러스터들의 상태를 결정한다. MySql에는 클러스터들의 내부 파라미터, config 정보가 있다. 이런 정보를 바탕으로 예상되는 클러스터의 상태와 실제 상태를 비교하여 상태를 결정한다.\n\n상태를 결정하면 MySql db에 저장된 rule table에 기반하여 세그먼트와 클러스터를 관리하게 된다.\n\n\n\n2-4-1. Rules\n\nrules은 historical segment들이 로드되고 드랍되어야 하는지에대한 것이며 다음과 같이 정의 된다.\n\n\n  Segment들이 어떻게 각기 다른 historical tier에 할당이 되어야 하는가\n  Segment들이 어떻게 replication 되어야 하는가\n  언제 Segment가 drop 되어야 하는가. 주로 시간으로 한다고 함\n\n\n그리고 이러한 rule은 MySQL db에 저장하고 load해서 사용함. 모든 Segment들에 매칭되는 Rule에 대해 적용한다.\n\n\n\n2-4-2.  Load Balancing\n\nproduction 환경에서는, historical nodes는 제한적인 리소스를 가지고 있기 때문에, segment들은 반드시 클러스터들에게 골고루 배분되어야 한다.\n\n이를 위해서 Druid는 a cost-based optimization procedure that takes into account the segment data source, recency,\nand size 를 사용한다고 한다.\n\n( 세부 알고리즘에 대한 설명은 없는데, 위 설명을 보자면 일반적으로 쿼리 패턴은 최근의 세그먼트를 자주 참조하는 경향이 있기 때문에, 최근 참조를 많이하는 segment는 복제를 더하는게 좋다고 논문에는 되어 있는데 맥락이 조금 이해가 안됨. )\n\n\n\n2-4-3. Replication\n\ncoordinator nodes는 historical nodes에게 같은 segment를 복제하라고 할 수 있다. 복제하는 숫자는 설정을 통해 계산이 가능하다.\n\n높은 레벨의 fault tolerance를 위해서는 많은 수의 레플리카를 설정하면 된다고 하면서, 사례를 논문에서는 제시함.\n\n\n\n2-4-4. Availability\n\ncoordinator nodes는 주키퍼와 MySql을 외부 의존성으로 가진다.\n\n\n  주키퍼 : historical nodes에 대한 클러스터 상태\n  MySql : Operation에 필요한 management information\n\n\n주키퍼가 unavailable 되면 현재 클러스터간 통신 상태 유지 ( 위 다른 nodes 와 동일 )\n\nMySql이 unavailable 되면 새로운 segment의 생성, 및 drop 불가능.\n\n\n\n\n3. Storage Format\n\n드루이드의 데이터 테이블은 시계열 이벤트이며 파티셔닝 된 segment들의 집합임.\n\n\n🔥 segment에 대한 정의는 아래 원문을 가져왔다.\n\nwe define a segment as a collection of rows of data that span some period of time.\nSegments represent the fundamental storage unit in Druid and replication and distribution are done at a segment level.\n\n\n\n드루이드의 데이터 포맷에서는 timestamp 컬럼이 반드시 있어야 된다. (논문에서는 always requires 라고 표현 함 ) 그 이유인 즉슨, 시간의 세분성을 기준으로 데이터의 볼륨과 시간의 범위를 결정하기 때문이다.\n\nSegment들은 데이터 소스의 식별자 역할을 한다. 새로운 segment가 추가 됨에 따라 time-interval, version이 증가되어 이 정보를 바탕으로 segment들의 최신버전???(freshness)를 반영함.\n\nDruid의 Segment는 컬럼 지향형이다. 그래서 aggregate에 best used 되어 있다. row 지향형보다 집계를 함에 있어 column 지향은 해당 컬럼만 로드하면 되기 때문에 더 효율적이다.\n\nDruid는 다중 컬럼 타입이 있고, 이는 다양한 데이터를 나타낸다고 함. 이러한 컬럼 타입에 의존하여 디스크에 데이터를 스토리징하는 비용이 줄어든다고 함.\n\n\n🔥 위 내용에 대한 예시와 ‘Indicaes for Filtering Data’ 그리고 ‘Storage Engine’에 대한 부분은 이해가 부족하여 차후 정리. OS 관련 지식이 조금 요구되는 부분.\n\n\n\n\n\n4. Query API\n\n\n  \n    {\n      \"queryType\" : \"timeseries\",\n      \"dataSource\" : \"wikipedia\",\n      \"intervals\" : \"2013-01-01/2013-01-08\",\n      \"filter\" : {\n      \"type\" : \"selector\",\n      \"dimension\" : \"page\",\n      \"value\" : \"Ke$ha\"\n    },\n      \"granularity\" : \"day\",\n      \"aggregations\" : [{\"type\":\"count\", \"name\":\"rows\"}]\n    }\n  \n\n\n위에서 설명한 각각의 노드들은 RestFul API가 존재하며, POST 메서드 형식에 body에 위와 같은 JSON 형태가 들어간다.\n\n그리고 드루이드는 조인 쿼리를 제공하지 않는다 .( A join query for Druid is not yet implemented )\n\n이는 컬럼 지향형 데이터 구조에서 Aggregation에 최적화 된 드루이드에서는 아래 두가지 이유로 조인을 제공하지 않는다.\n\n\n  Scaling join queries has been, in our professional experience,\na constant bottleneck of working with distributed databases.\n  The incremental gains in functionality are perceived to be\nof less value than the anticipated problems with managing\nhighly concurrent, join-heavy workloads.\n\n\n조인에 대해서 여러가지 전략이 있겠지만, Druid에서는 여러가지를 조인하는 것이 복잡한 메모리 관리가 필요한 큰 테이블이 요구되어, 메모리 관리의 복잡성이 증폭된다고 한다.\n\n🔥 결론 ! 조인을 제공하지 않는다.\n\n\n해당 부분은 아래 Druid 쿼리 관련한 내용을 함께 보면 좋을 듯.\n\nSQL · Apache Druid\n\n\n\n논문의 다음 내용은 성능과 Production Level에서의 드루이두 사용 결과에 대한 내용이다.\n\n일단 druid에 대한 주요 내용은 위 내용이 중요하다고 판단되어 우선 정리하였고, 빠진 개념들을 해당 글에 계속 보강하고자 함."

    },
  
    {

      "title"    : "(Sample Page) The Duality of Purpose and Work",
      "url"      : "/serial-communication",
      "content"  : "샘플입니다. \n\nPART ONE: HAVING FAITH IN GRAND DREAMS. Understanding the why of the work is the most important thing that’s needed when starting out. It’s so easy to become discouraged when you’re in the the thick of the weeds, when you need to do difficult or tedious. Motivation and discipline to do such work comes from having a bigger picture — a purpose.\n\nThis purpose comes from a place of realistic optimism and idealism. It’s a messy and bold goal. A tangible, yet implausible objective without conventional measure or perspective. There’s a lot wrong with the world — and a lot of people that will complain about it — but how can you improve it? What troubles are you willing to tackle head-on for the good of humanity?\n\n\n\n Person Making Clay Pot | Source\n\nHow to effectively go about doing what’s truly important.\n\n\n\nIt’s a lot to ask, to a point where most people don’t even push themselves to even attempt it. And those that do attempt to achieve something grand usually stop after failing only once or twice.\n\nThe reason so few people succeed in capturing the essence of a heartfelt purpose is a lack of faith. Having the faith in yourself that you’re truly capable of accomplishing that goal, no matter the obstacles, or opinions, or failures that you will inevitably face. It takes a fair amount of foolishness to ever succeed in accomplishing audacious.\n\nDon’t back out of a goal as soon as it looks like it’s going to fail — have more trust than doubt. Many people discuss the learning opportunities that reside in failure, but the truth of the matter is that gritting your teeth and persevering and eventually finding success is a far more enriching learning experience.\n\nIt is important to understand the context of where you currently stand in history. Research your contemporaries that have similar goals — especially if they’re currently doing far better than you are. Research the great thinkers and creators of the past, as well. Having this contextual model will not lead you to achievement, but will guide you in the right direction, like a compass.\n\nAt the same time, you must not be rigid with your objectives, either. Do not let pride get in your way — redirect your efforts if you need too. Being agile is far more intelligent than abandoning your efforts altogether and starting from square one.\n\nPART TWO: ELIMINATING EVERYTHING NON-ESSENTIAL. Simply understanding what you want to accomplish will not allow you to actually get it done. The what of work is doing the technical and nitty-gritty, which is the contrary of the initial motivating and lofty goals.\n\nThey both outstandingly require one another. Being only an idealist thinker won’t get anything done. While being only a busy-bee worker won’t get anything meaningful done.\n\nIt doesn’t matter if you are clueless when you’re first beginning — that’s normal. Mastery of any knowledge or skill requires only deliberate practice and enough time. Be humble enough to absorb the knowledge of everybody around you. Learn to constantly ask critical questions. Be resourceful with the amount of information you can find both locally and online.\n\nFigure out priorities, examine what work needs to be done as opposed to what’s just easy and non-essential filler. The next part is vital: look at your schedule and block out large chunks of time (four to seven hours) where you just work on what’s most important.\n\nYou cannot allow yourself to be interrupted by others, or distract yourself with the plethora of attention-grabbing media that’s currently at our disposal. This can be extremely difficult at first, but don’t be afraid to communicate with others that you’re busy with something and that you’ll talk to them later.\n\nSimilarly, push yourself to sticking to one tab or application open at a time. Stop yourself from constantly jump from one inquiry to another. Be mindful of how you’re using technology — take a break every once in awhile.\n\nA lot of people say they simply don’t have the time in their schedules to devote themselves so deeply to something. But the truth of the matter is that even if you have other responsibilities, you can fit this work into your calendar by understanding and eliminating the time you’re currently wasting — because we all waste time.\n\nCONCLUSION:\nI believe we’re all given the opportunity to achieve goodness in the world that’s beyond ourselves — whether it’s in small ways or big ways. We’re all born being good at something — technical or creative — and with an intense curiosity of the world we live in.\n\nIt is far too easy to veer off the path of pioneering, to instead be comfortable with letting life pass us by. I believe that we can make the conscious decision to change that — at any point in our lives — and instead aspire to greatness. All it takes is a little courage, and a lot of reckless abandon."

    },
  

  

  
]