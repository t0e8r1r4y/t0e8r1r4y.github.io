[
  
    {

      "title"    : "Hands-on.Create a tech blog",
      "url"      : "/git-blog",
      "content"  : "쉽지 않은 기술 블로그 만들기\n\n저는 스터디 그룹을 조그마하게 꾸리고있습니다. 스터디를 하면서 정리한 내용들을 블로그로 공개하고 ‘우리가 이런 일을 하고 이런거에 관심을 가지고 있어요.’라고 자랑하고 싶은(?) 니즈가 이미 오래전부터 있었습니다. 하지만 공통적으로 사용할 기술블로그를 만드는게 쉽지는 않았습니다.\n\n첫번째 시도는 노션 공개페이지를 활용하여 블로그를 만드는 것이었습니다. 노션은 사용하기 편리하고 스터디 구성원들 간 공유는 매우 편리하고 좋았습니다. 하지만 블로그 설명에 필요한 코드와 함께 연동하여 관리하기에는 다소 부족한 느낌이 있었습니다. 그리고 관리가 조금이라도 소홀해지면 사실 노트패드 같은 느낌으로 점점 쓰게되는 것 같았습니다. 완결성있는 블로그를 작성하기가 쉽지 않았습니다. 그런 의미에서 옵시디언도 같았습니다.\n\n두번째 시도는 노션에 작성한 글을 엮어서 미디엄 블로그를 만들어보자는 것이었습니다. 코드 관련된 부분은 여전히 어려움으로 남아있었지만, git 주소를 잘 남기고 관리한다면 보다 완결성있는 글을 쓸 수 있겠다고 판단하였습니다. 하지만 이 또한 어려움이 있었는데요. 노션에 비해서 문서 편집이 어려웠고, 코드를 삽입하는 것도 외부 의존성이 필요했습니다. 그리고 폰트나 꾸밈도 제한적이었습니다. 또 공통으로 사용할 구글 계정을 파고 해당 계정을 여러 환경에서 로그인을 시도하다보니, 보안적 이슈로 계정이 잠겨버리는 일도 있었습니다.\n\n\n\n\n노션 페이지를 그냥 블로그에 올려 버릴 수 없을까?\n\n여러 시도 끝에 가장 간단한 방법은 하나의 완성된 글을 노션에다 작성하고 이 페이지를 블로그에 바로 올려버리는 것이 간편하겠다는 생각이 들었습니다.\n\n노션은 스터디 구성원들이 수년간 사용한 경험이 있기에 다른 수단에 대한 러닝 커브가 가장 낮았습니다. 그리고 여타 다른 블로그들에 비해 api가 잘 되어있어 작성한 글을 자동으로 내려 받고, 이를 다른 블로그 플랫폼에 api를 호출하여 올리기에는 최적의 글쓰기 에디터라고 판단했습니다.\n\n이제 외부에 공개할 블로그 플랫폼을 선정하여야 했습니다. 티스토리, 브런치, 미디엄 등 다양한 블로그 플랫폼이 존재하였지만, 노션에서 추출한 내용을 업로드하기에 간편하다는 생각은 들지 않았습니다. 코드와 연동하여 가장 간편한 방법이 무엇일까 고민하였을 때 github blog가 최선이라는 생각이 들었습니다.\n\njeklly를 사용하면 github에서 모든 블로그 내용을 관리할 수 있고, 많은 공수없이 이미 만들어진 테마를 적용하여 빠르게 블로그 형태를 만들 수 있었습니다. 그리고 노션 api를 통해서 받아온 페이지 내용을 마크다운 파일 형태로 변경하여 블로그 포스트 경로에 추가해준 뒤 push만 하면 손쉽게 반영이 가능하여 발생을 위한 별도의 배포 파이프라인 구축도 불필요하였습니다.\n\n\n\n\n어떻게 구축할 것인가?!\n\n\n\n\n전체 그림은 위와 같습니다.\n\n\n  글 작성은 노션에서 진행합니다.\n  작성된 노션글은 파이썬으로 작성된 응용프로그램을 사용하여 api 호출을 통해 내용을 가져오고, 이를 마크다운으로 작성된 형태의 파일로 만듭니다.\n  현재는 로컬에서 git blog 프로젝트 경로에 포스트를 저장하는 경로에 저장하도록만 구현하였습니다. 그리고 로컬에서 미리 블로그 화면을 보고 검토를 한뒤 git에 push를 하도록 구축하였습니다.\n\n\n차후에는 운영룰을 정하고 응용프로그램을 aws와 같은 cloud 서비스에 올리므로서 자동으로 작성한 글이 post 되도록 할 예정입니다. 현재는 제 개인 기술 블로그를 위 내용으로 한번 구축해 봄으로서 필요한 내용을 아래에 정리하였습니다.\n\n\n\n\nGit Blog는 어떻게 만들 것인가\n\n제일먼저 필요한 것은 Git Blog의 테마라고 생각했습니다. 기본적으로 글을 어떻게 보여줄 것인지를 결정해야 했고, 노션에서 사용하는 강조 표시나 이런 부분들을 md 문법으로만 처리할 수 없기에 일부는 html style도 추가해야 했습니다. 이 부분에 대한 정확한 명세가 확정이 되어야 노션에서 추출한 게시글을 적절히 변경할 수 있다고 판단했습니다.\n\n\n\n\n\n저는 기본적인 목차형 블로그를 선택했습니다. 당연히 페이지 네이션도 적용을 해야된다고 생각했고, 글 상세 중 상단의 일부 내용을 보여 줄 수 있도록 만들었습니다. 몇개의 jeklly 테마를 조합해서 만들었습니다. 이미지의 경우 해당 레포지토리에 올릴 경우 용량의 한계가 있어 외부 cdn을 고려했습니다. 미디엄 블로그 등에 올린 포스트에서 주소만 잘 가져온다면 외부 블로그를 저장소로 사용할 수도 있다고 판단했고, 필요하다면 aws cloud front를 적절히 섞어 사용하면 될 것 같습니다.\n\n\n\n\n\n그리고 강조를 위한 부분은 div style을 별도로 정의하여 아래와 같이 만들었습니다. 이런 부분에서 front-end css에 대한 지식을 연마하는 기회도 되네요.\n\n\n\n\n\n그리고 코드와 관련된 부분은 highlight.js를 사용하였습니다. highlight.js는 적용도 간편하고 여러 디자인이 공개되어 있어 잘 적용한다면 아래와 같이 코드 가독성도 높일 수 있다고 생각했습니다.\n\n\n\n\n\n이렇게 페이지를 어떻게 노출할지 테마를 정하였다면 노션에서 데이터를 가져와서 변경을 할 준비가 되었습니다. 그리고 외부에 노출되는 것과 관련하여 ‘notion Google search console’ 설정을 해주어야합니다. 이를 통해 구글에서 검색 시 내 git blog 글이 노출될 되도록 합니다.\n\n\n\n\n응용 프로그램 개발에 필요한 것들\n\n응용 프로그램을 개발할 언어를 선택할 때 가장 중요하게 생각했던 부분은 api 호출을 간편하게 해줄 sdk나 lib가 공식적으로 있는지를 중요하게 생각했습니다. 노션 developer 공식 문서에는 js로 예제들이 적혀 있었지만 개인적으로 python 공식 sdk가 사용이 훨씬 편리하다고 판단하여 python으로 응용프로그램을 개발하기로 결정하였습니다.\n\n\n\n\n\n노션 api를 사용하기 위해서는 Integration 설정이 필요합니다. 노션 developer에서 키를 발급받아서 프로그램에서 api 호출 시 사용하면 됩니다. 그리고 페이지를 추출하기 위해서는 완결된 글 페이지에서 해당 Integration을 연동하는 작업도 필요합니다.\n\n\n\n\n\n위 작업을 완료한 뒤 api로 데이터를 호출하고 응답값을 분석하여 git blog에서 정의한 스타일에 맞게 적절히 변경을 해주어야 합니다. (api 분석에 대한 부분은 노하우가 생기면 별도의 글로 올리겠습니다.) 노션의 콜아웃 같은 경우 md로 적절히 변경하기 어렵기 때문에 응용 프로그램에서 적절히 변경을 해야합니다. 이런 변경이 완료되면 해당 파일을 저장할 때 git blog 프로젝트에서 포스트 내용을 저장하는 경로에 저장을 하였습니다. 이를 push하여 블로그에 글이 자동으로 반영되도록 하였습니다.\n\n\n\n\n어떤 운영룰을 만들면 좋을까?\n\n저는 작성한 글에 대한 리뷰를 운영룰을 정하는게 중요하다고 생각합니다. 공통으로 사용하는 블로그라면 전체적인 주제의 방향성을 유지하는 것도 중요하고, 코드 리뷰 처럼 게시글을 꼼꼼하게 리뷰하면서 서로의 역량을 강화하는 것도 중요하겠지요.\n\n\n\n\n\n블로그에 공개되는 내용은 _posts에 저장을 하고있는데 리뷰를 위해 _publication 경로를 만들어서 바로 올리지 않고, 한번 검수를 거쳐가도록 만들면 좋을 것 같습니다. PR을 올리면 글을 읽어보면서 바로바로 리뷰를 달고 이를 수정하여 _posts로 옮기면 좀 더 완성도 있는 글이 완성될 것 같습니다.\n\n그리고 차후에는 글감도 관리는 함께한다면 좀 더 완성도 있는 블로그가 되지 않을까요?\n\n\n\n\n마치며\n\n스터디 기술블로그를 구축하기 위한 고민과, 이를 먼저 개인블로그에 적용시켜보면서 얻은 내용을 간략하게 정리하였습니다. 해당 내용이 좀 더 발전이 된다면 해당글을 보완하고자 합니다.\n\n최근에는 스타트업 등에서도 기술력을 알리고자 이런 블로그를 만들고 관리하는 추세입니다. 이런 내용이 조금이나마 도움이 될 수 있으면 좋겠습니다."

    },
  
    {

      "title"    : "Hands-on.Add a login method",
      "url"      : "/login-part-two",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n\n  로그인 개선기 2부\n  다양한 로그인 방식 추가"

    },
  
    {

      "title"    : "Hands-on.Write procedures well in postgresql",
      "url"      : "/postgresql-procesure",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  프로시저에 대해서\n  업무에서 효율적으로 쓴 경험"

    },
  
    {

      "title"    : "Hands-on.Push Service. How to Do that?",
      "url"      : "/push-architecture",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  푸시에 대해서\n  푸시를 보내는 여러가지 방법 서칭"

    },
  
    {

      "title"    : "Hands-on.Getting started with Flutter",
      "url"      : "/flutter",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  앱개발\n  플러터의 이해"

    },
  
    {

      "title"    : "Hands-on.Manage image resources well with S3 and CloudFront and Waf",
      "url"      : "/images",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  s3 이미지 처리 관련 경험담\n  어찌하면 좋을까?"

    },
  
    {

      "title"    : "Hands-on.Improve Login Process",
      "url"      : "/login-part-one",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  로그인 개선기\n  로그인에 대해서"

    },
  
    {

      "title"    : "Theory.Is the CDC &amp; Debeziumev necessary for our organization?",
      "url"      : "/cdc-debezium",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  CDC란?\n  솔루션\n  사례\n  우리조직에 필요할까?"

    },
  
    {

      "title"    : "Hands-on.Introduction to React programming",
      "url"      : "/react-programing",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  프론트엔드 개발을 시작하면서\n  좌충우돌 front 개발기\n  제일먼저 찾아본 지식들\n  앞으로 프론트를 어떻게 해나갈 것인가?\n  feat. full stack이란 무엇인가?"

    },
  
    {

      "title"    : "Hands-on.Extract statistical information with JPA QueryDsl",
      "url"      : "/query-dsl",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  QueryDsl에 대해서\n  실무 이야기 잠깐해주고\n  QueryDsl 사용하면서 꿀팁\n  통계정보에 QueryDsl 적용\n  후기 장단점"

    },
  
    {

      "title"    : "Hands-on.Building AWS Infrastructure with Terraform",
      "url"      : "/terraform-first-copy",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  aws solution 중 사용한 것들과 그것들에 대한 특징 정리\n  terraform set up 요약\n  인프라 구조\n  적용하고나서 느낀점"

    },
  
    {

      "title"    : "Organizational Culture.Agile Process in Dev Org",
      "url"      : "/agile",
      "content"  : "TODO 입니다. \n\n\n\n목차\n\n  에자일에 대하여\n  우리 조직이 사용하는 프로세스와 솔루션 소개\n  나의 생각"

    },
  
    {

      "title"    : "Paper Reading. Apache Druid - A Real-time Analytical Data store",
      "url"      : "/apache_druid-paper-reading",
      "content"  : "1. 드루이드 개발 목적\n\n\n1-1.드루이드란? ( 논문의 결론 부분 )\n\n드루이는 다음을 충족하는 데이터 스토어이다.\n\n\n  분산\n  컬럼 지향형\n  리얼타임 분석 데이터 스토어 (처리하는 데이터를 drill-down하게 aggregates 할 수 있도록 설계 됨)\n  고성능, 쿼리 지연이 낮도록 설계 됨.\n  스트리밍 데이터 처리와 fault-tolerant를 지원함.\n\n\n\n\n1-2. 드루이드 개발 목적 ( 논문의 도입 부분과 문제 정의 부분 )\n\nDruid was originally designed to solve problems around ingesting and exploring large quantities of transactional events (log data). → OLAP\n\n드루이드는 분산,  컬럼 지향형, 리얼타임 분석 데이터 스토어 ( 쿼리 수행 속도가 더 빠른 )를 개발하고자 만들었다.\n\n이는 기존의 하둡이 혁신적이고 데이터 저장과 대용량의 데이터에 대한 접근은 우수하지만, 데이터에 대한 접근 속도(실시간 쿼리 등), 병렬로 수많은 데이터가 쏟아지는 경우 드루이드 개발자들의 니즈에 충족되지 못하였다고 함. ( Hadoop wasn’t going to meet our needs ).\n\n그리고 NoSql 아키텍처나 다른 솔루션들도 개발진이 원하는 수준이 아니거나, 비용타당성이 떨어짐.\n\n\n\n이러한 메타데이터는 Timestamp가 있고, Dimension 컬럼이 많고, Metric 컬럼이 많다.\n\n\n  원문  \n  This metadata is comprised of 3 distinct components. First, there is a timestamp column indicating when the edit\n  was made. Next, there are a set dimension columns indicating various attributes about the edit such as the page that was edited, the\n  user who made the edit, and the location of the user. Finally, there\n  are a set of metric columns that contain values (usually numeric)\n  that can be aggregated, such as the number of characters added or\n  removed in an edit.\n\n\n드루이드 개발진은 이러한 데이터를 drill-down하게 계산하고, aggregates 하는 것을 목표로 하였다. 그래서 논문의 결론을 한번더 언급하지만\n\n\n  분산\n  컬럼 지향적\n  리얼타임 데이터 분석 ( 리얼타임으로 drill-down하게 계산하고, 집계(agrregates) 하는 데이터 스토어\n\n\n이런 것을 만들고자 Druid를 만들었다.\n\n\n\n\n2. 드루이드 아키텍처\n\n\n\n2-1. Real-time Nodes\n\nReal-time Nodes는 실시간 데이터를 흡수하는 계층이면서 스트림에 쿼리 이벤트를 수행하는 노드라고 보임. ( 흡수하는 계층이면서 데이터 컨슈머 역할을 함 )\n\nReal-time Nodes에서는 이벤트들을 흡수하고 실시간 쿼리가 가능한데, 이를 위해서 in-memory index 버퍼를 유지함. ( 그리고 이를 위해서 JVM 내부 힙 기반의 버퍼를 사용한 row store를 가지고 있다고 하는데 요대목이 조금 이해가 안됨 )\n\n그리고 유지하는 index는 주기적으로 디스크에 유지하여 불변성을 유지함. 그리고 요때 컬럼지향형으로 데이터가 converting 된다.\n\n디스크에 저장된 index는 불변이고, 이를 논문에서는 persisted-indexer라고 명명함. 그리고 disk에 저장된 index들을 Off-heap memory에 불러온다.\n\n논문에서 제시한 도식처럼\n\n쿼리가 Real-time Node에 들어오면( Events indexed via these nodes are immediately available for querying ) in-memory index와 Off-heap memory and persisted indexex 둘 다 searching 해서 결과를 return한다.\n\n\n\n그리고 Real-time nodes는 일정 기간동안 영구적으로 저장된 index들을 하나의 불변의 block으로 묶고 ( 이를 segment라고 명명함 ) 영구적인 백업스토리지에 업로드를 한다.\n\n이런 백업 스토리지는 S3나 HDFS와 같은 파일시스템을 주로 쓰게되며, 이러한 행위를 드루이드에서는 deep storage라고 한다.\n\n이 과정을 요약하자면\n\n\n  실시간 스트리밍 데이터를 흡수하여 in-memory-index를 생성\n  in-memory-index를 컬럼 지향형 포맷으로 converting하고 Disk에 저장\n  일정 기간(some span of time) 모인 데이터를 묶어서 segment 생성\n  이 세그먼트를 백업 스토리지 저장 → deep store\n\n\n이 과정을 다시 논문 도식에 추가해보면…\n\n\n\n(데이터 indexing 방법에 대한 일반적인 내용도 조금 참고가 필요함)\n\n검색 데이터 색인 방법론\n\n\n\n2-1-1. Availability and Scalability\n\n위에서 Real-time-node는 컨슈머라는 표현을 썻는데… 논문의 맥락상 그 이야기는 여기 나온다. 컨슈머는 별도의 데이터 스트림을 생성해 줄 프로듀서가 필요하다.\n\n주로 이럴 때 kafka를 쓴다. 데이터 버스 역할로 카프카를 쓰는 이유는 데이터 버퍼의 역할과 데이터 손실 났을 때 카프카 broker에 저장 된 데이터 소스에서 데이터를 가져다 쓰기 때문에 손쉬운 복구가 가능하다는 의미로 보인다.\n\n\n\n논문에서 제시하는 카프카 + 드루이드 조합의 성능은 원문 그대로를 가지고 왔다\n\n🔥 원문 - In practice, this model has allowed one of the largest production Druid clusters to be able to consume raw data at approximately 500 MB/s (150,000 events/s or 2 TB hour).\n\n\n2-2. Historical Nodes\n\nHistorical Nodes는 불변 데이터의 읽기 일관성을 제공하는 노드다.\n\n\n\n해당 노드는 Deep Storage로 부터 세그먼트를 다운로드하거나 메모리로 로드하는 역할을 한다. Real-time-nodes 설명에서 동작하는 과정에서 위의 역할을 해주는 것이 historical nodes다.\n\n\n\n2-2-1. Tiers\n\nTiers는 Historical nodes의 그룹으로 정의한다. 티어들은 각각의 티어단위로 설정되는데,\n\nDifferent performance and fault-tolerance parameters can be set for each tier\n\n라고 원문에서 밝힌다. ( 원문을 한글로 더 좋게 번역을 못하겠음 )\n\n이렇게 하는 목적은 중요도에 따른 세그먼트들의 높고 낮은 우선순위 때문이다. 논문에서는 예시를 들어서 설명했는데, 액세스가 더 많이 되는 세그먼트들에게 더 큰 캐쉬 메모리를 할당하기 위한 목적 정도로 간단히 이해하면 될 듯.\n\n\n\n2-2-2. Availability\n\nHistorical node는 주키퍼를 통해 세그먼트를 로드 하거나 언로드하는 명령을 받는다. 그래서 주키퍼가 unavailable 상태가 되면 더 이상 새로운 세그먼트는 받지 못함. 그러나 서빙은 가능함.\n\nThis means that Zookeeper outages do not impact current data availability on historical nodes.\n\n\n\n2-3. Broker Nodes\n\nBroker Nodes는 real-time nodes와 historical nodes 사이에서 ‘쿼리 라우터’ 역할을 한다. 두가지 nodes 노드에게 질의하여 결과를 merge하는 역할을 하고 있다.\n\n\n\n2-3-1. Caching\n\n브로커 노드는 LRU 캐시 전략을 사용한다. ( 캐시 전략은 별도 링크 )\n\n\n\n\n  Broker nodes는 쿼리를 받으면 일단 세그먼트 셋에서 먼저 일치하는 것이 있는지 확인\n  일치하는 것이 있다면 다시 계산할 필요 없음\n  일치하는 것이 없다면, broker nodes는 historical nodes와 real time nodes에  쿼리를 각각 날림\n  broker nodes는 historical nodes에서 온 결과 값이 있으면 이를 cache로 저장 ( 이렇게 하는 이유는 real time nodes는 perpetually changing and caching the results is unreliable 하기 때문.\n\n\n\n\n2-3-2. Availability\n\n상태를 관리하는 주키퍼가 unavailable하게 되어도 쿼리가 가능하다. 기존에 클러스터와 통신이 정상이라고 하면 쿼리 수행을 하기 때문.\n\n\n\n2-4. Coordinator Nodes\n\nCoordinaotr Nodes는 드루이드 아키텍처 내부에서 ZooKeeper와 같은 역할을 하는 nodes이다. Historical nodes들의 분산 코디네이터 역할을 하며, historical nodes와 주키퍼 사이에서  load new data, drop outdated data, replicate data, and move data to load balance 이런 명령을 전달하는 역할을 한다.\n\nCoordinator Nodes들은 주키퍼에 의해 메인이 선출되고 나머지는 backup의 형태로 동작함\n\nCoordinator Nodes들은 주기적으로 현재 클러스터들의 상태를 결정한다. MySql에는 클러스터들의 내부 파라미터, config 정보가 있다. 이런 정보를 바탕으로 예상되는 클러스터의 상태와 실제 상태를 비교하여 상태를 결정한다.\n\n상태를 결정하면 MySql db에 저장된 rule table에 기반하여 세그먼트와 클러스터를 관리하게 된다.\n\n\n\n2-4-1. Rules\n\nrules은 historical segment들이 로드되고 드랍되어야 하는지에대한 것이며 다음과 같이 정의 된다.\n\n\n  Segment들이 어떻게 각기 다른 historical tier에 할당이 되어야 하는가\n  Segment들이 어떻게 replication 되어야 하는가\n  언제 Segment가 drop 되어야 하는가. 주로 시간으로 한다고 함\n\n\n그리고 이러한 rule은 MySQL db에 저장하고 load해서 사용함. 모든 Segment들에 매칭되는 Rule에 대해 적용한다.\n\n\n\n2-4-2.  Load Balancing\n\nproduction 환경에서는, historical nodes는 제한적인 리소스를 가지고 있기 때문에, segment들은 반드시 클러스터들에게 골고루 배분되어야 한다.\n\n이를 위해서 Druid는 a cost-based optimization procedure that takes into account the segment data source, recency,\nand size 를 사용한다고 한다.\n\n( 세부 알고리즘에 대한 설명은 없는데, 위 설명을 보자면 일반적으로 쿼리 패턴은 최근의 세그먼트를 자주 참조하는 경향이 있기 때문에, 최근 참조를 많이하는 segment는 복제를 더하는게 좋다고 논문에는 되어 있는데 맥락이 조금 이해가 안됨. )\n\n\n\n2-4-3. Replication\n\ncoordinator nodes는 historical nodes에게 같은 segment를 복제하라고 할 수 있다. 복제하는 숫자는 설정을 통해 계산이 가능하다.\n\n높은 레벨의 fault tolerance를 위해서는 많은 수의 레플리카를 설정하면 된다고 하면서, 사례를 논문에서는 제시함.\n\n\n\n2-4-4. Availability\n\ncoordinator nodes는 주키퍼와 MySql을 외부 의존성으로 가진다.\n\n\n  주키퍼 : historical nodes에 대한 클러스터 상태\n  MySql : Operation에 필요한 management information\n\n\n주키퍼가 unavailable 되면 현재 클러스터간 통신 상태 유지 ( 위 다른 nodes 와 동일 )\n\nMySql이 unavailable 되면 새로운 segment의 생성, 및 drop 불가능.\n\n\n\n\n3. Storage Format\n\n드루이드의 데이터 테이블은 시계열 이벤트이며 파티셔닝 된 segment들의 집합임.\n\n\n🔥 segment에 대한 정의는 아래 원문을 가져왔다.\n\nwe define a segment as a collection of rows of data that span some period of time.\nSegments represent the fundamental storage unit in Druid and replication and distribution are done at a segment level.\n\n\n\n드루이드의 데이터 포맷에서는 timestamp 컬럼이 반드시 있어야 된다. (논문에서는 always requires 라고 표현 함 ) 그 이유인 즉슨, 시간의 세분성을 기준으로 데이터의 볼륨과 시간의 범위를 결정하기 때문이다.\n\nSegment들은 데이터 소스의 식별자 역할을 한다. 새로운 segment가 추가 됨에 따라 time-interval, version이 증가되어 이 정보를 바탕으로 segment들의 최신버전???(freshness)를 반영함.\n\nDruid의 Segment는 컬럼 지향형이다. 그래서 aggregate에 best used 되어 있다. row 지향형보다 집계를 함에 있어 column 지향은 해당 컬럼만 로드하면 되기 때문에 더 효율적이다.\n\nDruid는 다중 컬럼 타입이 있고, 이는 다양한 데이터를 나타낸다고 함. 이러한 컬럼 타입에 의존하여 디스크에 데이터를 스토리징하는 비용이 줄어든다고 함.\n\n\n🔥 위 내용에 대한 예시와 ‘Indicaes for Filtering Data’ 그리고 ‘Storage Engine’에 대한 부분은 이해가 부족하여 차후 정리. OS 관련 지식이 조금 요구되는 부분.\n\n\n\n\n\n4. Query API\n\n\n  \n    {\n      \"queryType\" : \"timeseries\",\n      \"dataSource\" : \"wikipedia\",\n      \"intervals\" : \"2013-01-01/2013-01-08\",\n      \"filter\" : {\n      \"type\" : \"selector\",\n      \"dimension\" : \"page\",\n      \"value\" : \"Ke$ha\"\n    },\n      \"granularity\" : \"day\",\n      \"aggregations\" : [{\"type\":\"count\", \"name\":\"rows\"}]\n    }\n  \n\n\n위에서 설명한 각각의 노드들은 RestFul API가 존재하며, POST 메서드 형식에 body에 위와 같은 JSON 형태가 들어간다.\n\n그리고 드루이드는 조인 쿼리를 제공하지 않는다 .( A join query for Druid is not yet implemented )\n\n이는 컬럼 지향형 데이터 구조에서 Aggregation에 최적화 된 드루이드에서는 아래 두가지 이유로 조인을 제공하지 않는다.\n\n\n  Scaling join queries has been, in our professional experience,\na constant bottleneck of working with distributed databases.\n  The incremental gains in functionality are perceived to be\nof less value than the anticipated problems with managing\nhighly concurrent, join-heavy workloads.\n\n\n조인에 대해서 여러가지 전략이 있겠지만, Druid에서는 여러가지를 조인하는 것이 복잡한 메모리 관리가 필요한 큰 테이블이 요구되어, 메모리 관리의 복잡성이 증폭된다고 한다.\n\n🔥 결론 ! 조인을 제공하지 않는다.\n\n\n해당 부분은 아래 Druid 쿼리 관련한 내용을 함께 보면 좋을 듯.\n\nSQL · Apache Druid\n\n\n\n논문의 다음 내용은 성능과 Production Level에서의 드루이두 사용 결과에 대한 내용이다.\n\n일단 druid에 대한 주요 내용은 위 내용이 중요하다고 판단되어 우선 정리하였고, 빠진 개념들을 해당 글에 계속 보강하고자 함."

    },
  
    {

      "title"    : "(Sample Page) The Duality of Purpose and Work",
      "url"      : "/serial-communication",
      "content"  : "샘플입니다. \n\nPART ONE: HAVING FAITH IN GRAND DREAMS. Understanding the why of the work is the most important thing that’s needed when starting out. It’s so easy to become discouraged when you’re in the the thick of the weeds, when you need to do difficult or tedious. Motivation and discipline to do such work comes from having a bigger picture — a purpose.\n\nThis purpose comes from a place of realistic optimism and idealism. It’s a messy and bold goal. A tangible, yet implausible objective without conventional measure or perspective. There’s a lot wrong with the world — and a lot of people that will complain about it — but how can you improve it? What troubles are you willing to tackle head-on for the good of humanity?\n\n\n\n Person Making Clay Pot | Source\n\nHow to effectively go about doing what’s truly important.\n\n\n\nIt’s a lot to ask, to a point where most people don’t even push themselves to even attempt it. And those that do attempt to achieve something grand usually stop after failing only once or twice.\n\nThe reason so few people succeed in capturing the essence of a heartfelt purpose is a lack of faith. Having the faith in yourself that you’re truly capable of accomplishing that goal, no matter the obstacles, or opinions, or failures that you will inevitably face. It takes a fair amount of foolishness to ever succeed in accomplishing audacious.\n\nDon’t back out of a goal as soon as it looks like it’s going to fail — have more trust than doubt. Many people discuss the learning opportunities that reside in failure, but the truth of the matter is that gritting your teeth and persevering and eventually finding success is a far more enriching learning experience.\n\nIt is important to understand the context of where you currently stand in history. Research your contemporaries that have similar goals — especially if they’re currently doing far better than you are. Research the great thinkers and creators of the past, as well. Having this contextual model will not lead you to achievement, but will guide you in the right direction, like a compass.\n\nAt the same time, you must not be rigid with your objectives, either. Do not let pride get in your way — redirect your efforts if you need too. Being agile is far more intelligent than abandoning your efforts altogether and starting from square one.\n\nPART TWO: ELIMINATING EVERYTHING NON-ESSENTIAL. Simply understanding what you want to accomplish will not allow you to actually get it done. The what of work is doing the technical and nitty-gritty, which is the contrary of the initial motivating and lofty goals.\n\nThey both outstandingly require one another. Being only an idealist thinker won’t get anything done. While being only a busy-bee worker won’t get anything meaningful done.\n\nIt doesn’t matter if you are clueless when you’re first beginning — that’s normal. Mastery of any knowledge or skill requires only deliberate practice and enough time. Be humble enough to absorb the knowledge of everybody around you. Learn to constantly ask critical questions. Be resourceful with the amount of information you can find both locally and online.\n\nFigure out priorities, examine what work needs to be done as opposed to what’s just easy and non-essential filler. The next part is vital: look at your schedule and block out large chunks of time (four to seven hours) where you just work on what’s most important.\n\nYou cannot allow yourself to be interrupted by others, or distract yourself with the plethora of attention-grabbing media that’s currently at our disposal. This can be extremely difficult at first, but don’t be afraid to communicate with others that you’re busy with something and that you’ll talk to them later.\n\nSimilarly, push yourself to sticking to one tab or application open at a time. Stop yourself from constantly jump from one inquiry to another. Be mindful of how you’re using technology — take a break every once in awhile.\n\nA lot of people say they simply don’t have the time in their schedules to devote themselves so deeply to something. But the truth of the matter is that even if you have other responsibilities, you can fit this work into your calendar by understanding and eliminating the time you’re currently wasting — because we all waste time.\n\nCONCLUSION:\nI believe we’re all given the opportunity to achieve goodness in the world that’s beyond ourselves — whether it’s in small ways or big ways. We’re all born being good at something — technical or creative — and with an intense curiosity of the world we live in.\n\nIt is far too easy to veer off the path of pioneering, to instead be comfortable with letting life pass us by. I believe that we can make the conscious decision to change that — at any point in our lives — and instead aspire to greatness. All it takes is a little courage, and a lot of reckless abandon."

    },
  

  

  
]